---
title: 'Practical Machine Learning: Exercise'
author: "Julian Jordi"
date: "2021-09"
output: html_document
---

## Summary
We analyze the Human Actitivty Recognition dataset provided by groupware  [here](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har) to predict how well a particular exercise was performed.

The final model chosen is a ...

## The Data
Let's have a first look at the data
```{r message=FALSE, warning=FALSE}
library(tidyverse); library(caret); library(corrplot); library(parallel); library(doParallel)

set.seed(1000)
```
```{r}
training_raw = read_csv("pml-training.csv")
training_raw$classe = as.factor(training_raw$classe)
```


So we have a total of 160 columns and 19622 records. In other words, 159 potential predictors for the variable 'classe', which we want to predict.
We have enough data to set aside a portion as a validation set. This will allow us to validate different models before deciding which on to use for the final prediction.

```{r}

partition = createDataPartition(y=training_raw$classe, p=0.8, list = F)
training = training_raw %>% slice(partition)
validation = training_raw %>% slice(-partition)
dim(training)
dim(validation)
```

Before we start training models, let's look if there are some obvious predictors which we do not need.

First of all, for many predictors, we have almost no measurements. Any prediction based on these would be highly unreliable. We therefore collect these columns for exclusion. This throws away 100 of the 159 possible predictors!

```{r}
min_required_observations = 1000  # we require at least 1000 valid measurements for a predictor to be included
ignored_columns = which(colSums(!is.na(training_raw) & training_raw != "") < min_required_observations)
length(ignored_columns)
```

Second, we do not care so much about any of the 'timestamp' columns. Event though it is entirely possible that the the day, or time of day, when a particular exercise is performance, does correlate with the _quality_ with which it is performed, this should also be predictable by the other, concrete, measurements. In other words, we should get a good prediction even without these columns.

Also, notice the first column in the dataset is just an index column which holds no predictive value. We exclude it also.

```{r}
timestamp_columns = names(training_raw) %>% grep(pattern = "timestamp")
index_column = 1
ignored_columns = c(ignored_columns, timestamp_columns, index_column)
training = training %>% select(!all_of(ignored_columns))
validation = validation %>% select(!all_of(ignored_columns))
```

Also, if we find very high correlation between any pair of the remaining predictors (at least numerical ones), we might exclude some redundant ones and avoid overfitting.

Indeed, we do find some pockets of highly correlated features:

```{r}
numeric_columns = which(sapply(training, is.numeric))
correlation_matrix = cor(training[,numeric_columns])
corrplot(correlation_matrix, type = "lower", tl.cex = 0.5, tl.col = 'black', title = "Correlation among Predictors")
```

Caret's `findCorrelation` function gives us the columns which we should remove to reduce pair-wise correlations. At this point, it is unclear if removing these columns make the model better and/or faster. To compare both, we keep a separate training and validation set for these predictors.

```{r}
columns_to_remove = findCorrelation(correlation_matrix, exact=TRUE)
print(paste("removing columns", paste(columns_to_remove, collapse = ", "), "due to high correlation (>0.9) with one other columns"))
training_uncorrelated = training %>% select(!all_of(columns_to_remove))
validation_uncorrelated = validation %>% select(!all_of(columns_to_remove))
```
Let us now build and compare some models with these training and validation sets.

## Model Building
This kind of problem seems to be well-suited to random forest type methods. The reasoning for this is that such forests can capture very diverse ...TODO

### Training a Random Forest
We use the caret package to train the model, as this allows us to profit from the built-in parameter tuning. In particular, caret will zero in on the most important predictors to avoid overfitting.

To shorten training time, we use parallel processing and use 5-fold cross-validation as resampling technique. This is less computationally intensive than the default bootstrap method. But, as it turns out, the accuracy of the produced model is still quite satisfactory
```{r, cache=TRUE}
trainCtl = trainControl(allowParallel = T) # use parallel computing, otherwise standard training configuration
trainCtlCv = trainControl(allowParallel = T, method = "cv", number = 5) # use 5-fold cross validation for resampling
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

# system.time(fit <- train(x=x, y=y, method="rf", trControl = trainCtl)) # this takes a long time, and the resulting model is not better than with cross-validation
system.time(random_forest_fit <- train(classe ~ ., method="rf", data = training_uncorrelated, trControl = trainCtlCv))
random_forest_fit
```


### Training Boosted Decision Trees
This model uses the `gbm` package to build a boosted model based on decision trees.
```{r, cache=TRUE}
registerDoParallel(cluster)

system.time(boosted_trees_fit <- train(classe ~ ., method="gbm", data=training_uncorrelated, verbose=F, trControl = trainCtl))
stopCluster(cluster)
registerDoSEQ()
boosted_trees_fit
```

## Model Selection
We now pick the most promising model among the three. We do this by comparing their performance on the validation set.


## Use of Cross Validation

In-Sample error....

```{r pressure, echo=FALSE}
plot(pressure)
```

## Results
When applying to test set